{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21adf9a9-f4bc-46e3-a505-b88844207c50",
   "metadata": {},
   "source": [
    "# Messing with Attention\n",
    "\n",
    "Just trying some basic tensor operations with PyTorch's attention implementation.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "\n",
    "See also, transformers pseudocode: https://arxiv.org/abs/2207.09238\n",
    "\n",
    "Other links:\n",
    "\n",
    " - [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    " - [miniGPT](https://github.com/karpathy/minGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22855821-5aa2-4acd-b55e-37c78a5a8011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e37de6-4c80-4161-a7e7-dd365c48a6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(9, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.nn.Embedding(\n",
    "    num_embeddings=9,\n",
    "    embedding_dim=768,\n",
    ")\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41105eb6-0d95-4e75-92f2-6c838ff43fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.nn.MultiheadAttention(\n",
    "    embed_dim=768,\n",
    "    num_heads=2,\n",
    ")\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cd5373-4390-4583-96f1-7d96f0b0b7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.IntTensor([1])).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44923374-4950-410f-a39d-e56e15fbca7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.IntTensor([1, 2])).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570fb4ea-bf98-4b05-a725-90651730c4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.IntTensor([0, 5, 8])).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c548b2db-1f67-48f3-8c8a-09713d54abc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = torch.IntTensor([0, 5, 8])\n",
    "x = e(input_sequence)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d818d4e-d5e3-47da-824f-c3599dd55181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 768]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = x\n",
    "k = x\n",
    "v = x\n",
    "attn_outputs, attn_output_weights = attn(q, k, v)\n",
    "attn_outputs.size(), attn_output_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1700f9a1-31f6-411c-b911-ff6616c716d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.out_proj.weight.size()\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5aba51-733d-4663-a402-36ddc492b3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'embed_dim', 'kdim', 'vdim', '_qkv_same_embed_dim', 'num_heads', 'dropout', 'batch_first', 'head_dim', 'bias_k', 'bias_v', 'add_zero_attn'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b55903-0cf5-49f8-89d7-df848e6694f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn._parameters[\"in_proj_weight\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db8d187c-76df-4664-85a0-e114754cf75e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2304 / 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfceec55-cafa-43c3-bf09-461705fb6d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.in_proj_weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdcf0593-dddb-49d7-b93f-31eb05c29103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.head_dim, 768 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d60e20-e65f-4cce-aea2-92efe5187b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aeeae56-4701-447c-b215-3b6765012e15",
   "metadata": {},
   "source": [
    "### Decoder-only transformer implementation from Andrej Karpathy\n",
    "\n",
    "Borrowed and tweaked from here: \n",
    "https://colab.research.google.com/drive/1SiF0KZJp75rUeetKOWqpsA8clmHP6jMg?usp=sharing#scrollTo=wW1-8xqswRYg\n",
    "\n",
    "I suspect this is from miniGPT or nanoGPT, I didn't check. (MIT license)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a88c80e8-0abf-4c6b-a8fb-c9c8480441c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, \"Heads need to cleanly divide into n_embd\"\n",
    "        # this assertion produces the \"embed_dim must be divisible by num_heads\" in nn.MultiheadAttention\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        # this is the same approach nn.MultiheadAttention uses for the in_proj_weight parameter if the key, query, and value have the same dimension\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.nonlin = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.nonlin(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # these are default GPT-2 hyperparameters\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    bias: bool = False\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.config.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(\n",
    "            x[:, -1, :]\n",
    "        )  # note: only returning logits at the last time step (-1), output is 2D (b, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3042f11-b6ef-4b20-9eec-75a9d3cf5522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m619\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m gpt \u001b[38;5;241m=\u001b[39m GPT(\u001b[43mconfig\u001b[49m)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(gpt\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(619)\n",
    "\n",
    "# vocab size is 2, so we only have two possible tokens: 0,1\n",
    "vocab_size = 2\n",
    "# context length is 3, so we take 3 bits to predict the next bit probability\n",
    "context_length = 3\n",
    "config = GPTConfig(\n",
    "    block_size=16,\n",
    "    vocab_size=4,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=16,\n",
    "    bias=False,\n",
    ")\n",
    "gpt = GPT(config)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830962d-8158-4cb2-a813-d1a64e352e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    logits = gpt(X)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(i, loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratchpad-finetune",
   "language": "python",
   "name": "scratchpad-finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
